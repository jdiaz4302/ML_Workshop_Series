{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization or **training** in the process of making your model better with respect to your training data.\n",
    "\n",
    "**Simple models** with few parameters can often be trained very quickly, this is because you can either directly compute their optimal model weights or because, due to their small number of parameters, the brute force optimizations can be done quickly. More **complex models**, such as neural networks, have far too many weights to directly solve for, so we always rely on a brute force implementations, which, when coupled with a lot of training data, can sometimes take hours or days to train.\n",
    "\n",
    "In order to train a model we must specify both the **loss function** and the (brute-force) **optimization method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A familiar one\n",
    "As stated above, trianing is about making the model better. Well, what dictates what is \"better\"? The loss function does!\n",
    "\n",
    "A very familiar loss function is the one used in least squares regression:\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\Big(y_i - \\hat{y_i}\\Big)$$\n",
    "\n",
    "Where:<br>\n",
    "$y_i$ = the observed $y$<br>\n",
    "$\\hat{y_i}$ = the predicted $y$<br><br><br>\n",
    "\n",
    "#### How this can lead to changes in the model\n",
    "As the one above does, usually a loss function specifies how bad your model currently is and then you can change your model weights to change the \"badness\" value that the loss function provides. This can be done because your prediction is simply a function of the model weights:\n",
    "\n",
    "$$\\hat{y_i} = \\hat{a}x_i+\\hat{b}$$\n",
    "\n",
    "Where:<br>\n",
    "$\\hat{a}$ = your current estimate for the intercept<br>\n",
    "$\\hat{b}$ = your current estimate for the slope\n",
    "\n",
    "More generally, a loss function is a type of objective function - a function you somehow want to optimize with respect to. Other fields of AI, such as robotics will more commonly use reward functions that reward a robot for performing the desired task.<br><br><br>\n",
    "\n",
    "#### For logistic regression\n",
    "In logistic regression, the loss function is:\n",
    "\n",
    "$$\\sum_{i=1}^{n} -\\big(y_ilog\\big(\\hat{y_i}\\big) + \\big(1-y_i\\big)log\\big(1-\\hat{y_i}\\big)\\big)$$\n",
    "\n",
    "Where your model predictions ($\\hat{y_i}$) now equal:\n",
    "$$\\frac{1}{1 + e^{\\hat{a}x_i + \\hat{b}}}$$\n",
    "\n",
    "Remember that this is just mapping your linear regression onto the S-curve bound between $0$ and $1$. Visualized here: http://www.wolframalpha.com/input/?i=plot+(1%2F(1%2Be%5E(-y)))+from+y+%3D+-10+to+y+%3D+10\n",
    "\n",
    "To make **sense of this loss function**, imagine the situation where you predict that $\\hat{y} = 0$ but $y = 1$, then you get:<br><br>\n",
    "$$-1\\big(1*log\\big(0\\big) + \\big(1 - 1\\big)log\\big(1-0\\big)\\big)$$\n",
    "Which simplifies to:<br>\n",
    "$$-1\\big(-\\infty + 0\\big)$$\n",
    "\n",
    "Which is just $\\infty$.<br>\n",
    "\n",
    "Infinity is more a concept than a number, so it would be hard to optimize this, but, thankfully, the model should never return $\\hat{y} = 0$ (because it would require $\\hat{a}x_i+\\hat{b} = -\\infty$). This does, however, show that if your model is absolutely wrong, that the loss will be very large, and while $\\hat{y}$ wont equal zero, it can definitely approach it.<br>\n",
    "\n",
    "It can similarly be shown that if you predict $\\hat{y} = 1$ when $y = 0$ that the loss will also equal $\\infty$ (The log operation on the right side of the equation will return a $-\\infty$ while the left side will return $0$)<br>\n",
    "\n",
    "It may be worth emphasizing that loss functions are not necessarily specific to models and can be broadly applicable to many models. This is because they only give you an idea of how bad your predictions are in a continuous and discrete case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizating the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of the difference between the loss function (height) mapped against two model weights looks like for a simple model such as linear regression (left) and a complex model such as a neural network (right): https://image.slidesharecdn.com/mlconf2015-sf-anandkumar-151114002155-lva1-app6892/95/animashree-anandkumar-electrical-engineering-and-cs-dept-uc-irvine-at-mlconf-sf-111315-10-638.jpg?cb=1447460604\n",
    "\n",
    "Since we often can't directly compute the optimal solution for complex-model loss functions. We usually use gradient-based methods. A gradient is simply an n-dimensional derivative and it gives you the n-dimensional direction of greatest decrease.\n",
    "\n",
    "So, while we may not know what the optimal solution is, we have a good idea of what direction we need to go in order to get closer to it. This process is done with over many iterations (steps) with a specified learning rate (step size) - some optimization methods will help you determine this learning rate which helps get rid of one hyperparameter.\n",
    "\n",
    "We are able to determine these gradients/derivatives because at the end of the day, even the most complex models are based on simpler subequations of adding and multiplying; it is therefore a matter of calculating simpler derivatives using the chain rule from calculus.\n",
    "\n",
    "Here is a video showing a gradient-based optimization: https://www.youtube.com/watch?v=kJgx2RcJKZY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data# Impor \n",
    "tor_df = pd.read_csv(\"tor_train_set.csv\")\n",
    "\n",
    "\n",
    "# Get the outcomes\n",
    "tornado_outcome = tor_df.iloc[:, [2]]\n",
    "\n",
    "# Convert the pandas column to a ndarray and then into a FloatTensor\n",
    "train_outcome_Variable = Variable(torch.from_numpy(tornado_outcome.values).float())\n",
    "\n",
    "\n",
    "# Get the predictors\n",
    "tornado_predictors = tor_df.iloc[:, 3:]\n",
    "\n",
    "# Make the validation set predictors into a numpy array\n",
    "train_predictors_Variable = Variable(torch.from_numpy(tornado_predictors.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test set data# Impor \n",
    "test_df = pd.read_csv(\"tor_test_set.csv\")\n",
    "\n",
    "\n",
    "# Get the outcomes\n",
    "test_outcome = test_df.iloc[:, [2]]\n",
    "\n",
    "# Convert the pandas column to a ndarray and then into a FloatTensor\n",
    "test_outcome_Variable = Variable(torch.from_numpy(test_outcome.values).float())\n",
    "\n",
    "\n",
    "# Get the test set predictors\n",
    "test_predictors = test_df.iloc[:, 3:]\n",
    "\n",
    "# Make the test set predictors into a numpy array\n",
    "test_predictors_Variable = Variable(torch.from_numpy(test_predictors.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prop_dam_to_binary(property_damage_values):\n",
    "    \n",
    "    # This function will convert continuous property damage values to binary values defining whether\n",
    "        # or not a tornado caused any damage\n",
    "    # property_damage_values = a PyTorch Tensor containing property damage values\n",
    "    # Returns as PyTorch Tensor of binary values\n",
    "    \n",
    "    \n",
    "    # Get the Tensor as a ndarray \n",
    "    prop_dam_array = property_damage_values.data.numpy()\n",
    "    \n",
    "    # For-loop to convert to binary\n",
    "    for i in list(range(len(prop_dam_array))):\n",
    "        \n",
    "        if (prop_dam_array[i] == prop_dam_array.min())[0]:\n",
    "            \n",
    "            prop_dam_array[i] = 0\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            prop_dam_array[i] = 1\n",
    "     \n",
    "    # Convert ndarray to Tensor\n",
    "    prop_dam_Tensor = Variable(torch.from_numpy(prop_dam_array))\n",
    "    \n",
    "    # Return Tensor\n",
    "    return(prop_dam_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data\n",
    "train_Y_binary = convert_prop_dam_to_binary(train_outcome_Variable)\n",
    "\n",
    "# And the test data\n",
    "test_Y_binary = convert_prop_dam_to_binary(test_outcome_Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictors_Variable.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.logistic_layer = nn.Sequential(nn.Linear(51, 1),\n",
    "                                          nn.Sigmoid())\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        logistic_output = self.logistic_layer(x)\n",
    "        return(logistic_output)\n",
    "\n",
    "\n",
    "# Make it\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Optimizing options\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list  = []\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = classifier(train_predictors_Variable)\n",
    "    test_predictions = classifier(test_predictors_Variable)\n",
    "    \n",
    "    loss = loss_function(predictions, train_Y_binary)\n",
    "    test_loss = loss_function(test_predictions, test_Y_binary)\n",
    "    \n",
    "    loss_list.append(loss.data[0])\n",
    "    test_loss_list.append(test_loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list, label = 'train')\n",
    "plt.plot(test_loss_list, label = 'test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_prediction_list  = []\n",
    "\n",
    "test_predictions = classifier(test_predictors_Variable)\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    plain_prediction = test_predictions[i].data.numpy()[0]\n",
    "    if plain_prediction < 0.5:\n",
    "        plain_prediction_list.append(0)\n",
    "    else:\n",
    "        plain_prediction_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y_binary_list = test_Y_binary.data.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(plain_prediction_list, test_Y_binary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(plain_prediction_list, test_Y_binary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(51, 26),\n",
    "                                          nn.ReLU())\n",
    "        self.output_layer = nn.Sequential(nn.Linear(26, 1),\n",
    "                                          nn.Sigmoid())\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_output = self.hidden_layer(x)\n",
    "        final_output = self.output_layer(hidden_output)\n",
    "        return(final_output)\n",
    "\n",
    "\n",
    "# Make it\n",
    "classifier = NeuralNetwork()\n",
    "\n",
    "# Optimizing options\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = classifier(train_predictors_Variable)\n",
    "    test_predictions = classifier(test_predictors_Variable)\n",
    "    \n",
    "    loss = loss_function(predictions, train_Y_binary)\n",
    "    test_loss = loss_function(test_predictions, test_Y_binary)\n",
    "    \n",
    "    loss_list.append(loss.data[0])\n",
    "    test_loss_list.append(test_loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list, label = 'train')\n",
    "plt.plot(test_loss_list, label = 'test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_prediction_list = []\n",
    "\n",
    "test_predictions = classifier(test_predictors_Variable)\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    plain_prediction = test_predictions[i].data.numpy()[0]\n",
    "    if plain_prediction < 0.5:\n",
    "        plain_prediction_list.append(0)\n",
    "    else:\n",
    "        plain_prediction_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(plain_prediction_list, test_Y_binary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(plain_prediction_list, test_Y_binary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
