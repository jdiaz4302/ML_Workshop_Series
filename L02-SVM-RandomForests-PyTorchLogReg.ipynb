{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same stuff from last time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10, 10)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the csv\n",
    "climate_essay_df = pd.read_csv('data_noID.csv')\n",
    "\n",
    "# How big is this dataframe\n",
    "climate_essay_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_df = climate_essay_df[climate_essay_df['trt1'] == 1]\n",
    "subsetted_df = subsetted_df[['trt1', 'don', 'essay']]\n",
    "subsetted_df = subsetted_df.reset_index(drop = True)\n",
    "subsetted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_df['big_donator'] = (subsetted_df['don'] > 8.5)\n",
    "subsetted_df['big_donator'] = subsetted_df['big_donator'].astype(int)\n",
    "subsetted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train (in-sample) and test (out-of-sample) sets\n",
    "train_df, test_df = train_test_split(subsetted_df, \n",
    "                                     test_size = 0.2,       # 80/20 train/test split\n",
    "                                     random_state = 123)    # Making sure everyone gets the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Panda's formal dataframe column into a standard vector of strings\n",
    "train_essays = train_df['essay'].values\n",
    "test_essays = test_df['essay'].values\n",
    "\n",
    "# Creating a CountVectorizer object\n",
    "word_counter = CountVectorizer()\n",
    "\n",
    "# Fitting this word-counter on our train essays\n",
    "word_counter.fit(train_essays)\n",
    "\n",
    "# Transforming the train and test set essays into the word count form\n",
    "test_word_counts = word_counter.transform(test_essays)\n",
    "train_word_counts = word_counter.transform(train_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tSNE is similar to PCA in that it reduces dimensionality of your data, however they are different. Notably, PCA outputs a function to transform new data, while tSNE does not. Additionally, they reduce dimensions in different ways: PCA reduces in a way that maintains the variability of the original data, while tSNE reduces in a way that maintains distances between points. Additionally tSNE is a nonlinear dimensionality reduction, where PCA is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = test_df['big_donator'].values\n",
    "train_Y = train_df['big_donator'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type, new_dim, kernel, C_value, min_split, max_depth, n_estimators):\n",
    "    \n",
    "    # Set up\n",
    "    if new_dim < 999:\n",
    "        dim_reduction = PCA(n_components = new_dim, random_state=123)\n",
    "        dim_reduction.fit(train_word_counts.todense())\n",
    "        train_reduced = dim_reduction.transform(train_word_counts.todense())\n",
    "        test_reduced = dim_reduction.transform(test_word_counts.todense())\n",
    "    else:\n",
    "        train_reduced = train_word_counts\n",
    "        test_reduced = test_word_counts\n",
    "    \n",
    "    # Fitting model\n",
    "    if (model_type == 'svm'):\n",
    "        model = SVC(kernel = kernel, C = C_value, random_state=123)\n",
    "        \n",
    "    elif (model_type == 'decision_tree'):\n",
    "        model = DecisionTreeClassifier(min_samples_split = min_split,\n",
    "                                       max_depth = max_depth, random_state = 123)\n",
    "        \n",
    "    elif (model_type == 'random_forest'):\n",
    "        model = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth,\n",
    "                                       min_samples_split = min_split, random_state = 123)\n",
    "    elif (model_type == 'logistic_regression'):\n",
    "        model = LogisticRegression()\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model type. Valid models include: 'svm', 'decision_tree'\" \\\n",
    "              \"'random_forest', 'logistic_regression'\")\n",
    "    model.fit(X = train_reduced,\n",
    "              y = train_Y)\n",
    "    \n",
    "    # Magic plotting code - Only tested with new_dim = 2 \n",
    "    if model_type == 'svm':\n",
    "        if new_dim == 2:\n",
    "            h = 0.2\n",
    "            x_min, x_max = train_reduced[:,0].min() - 1, train_reduced[:, 0].max() + 1\n",
    "            y_min, y_max = train_reduced[:,1].min() - 1, train_reduced[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(\n",
    "                np.arange(x_min, x_max, h),\n",
    "                np.arange(y_min, y_max, h))\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            plt.contourf(xx,yy,Z,cmap=plt.cm.viridis, alpha=0.8)\n",
    "            plt.scatter(train_reduced[:,0],train_reduced[:,1],c=train_Y, cmap = plt.cm.viridis)\n",
    "            plt.show()\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"TRAIN SET\")\n",
    "    train_predictions = model.predict(train_reduced)\n",
    "    train_accuracy = accuracy_score(train_predictions, train_Y)\n",
    "    print('The training set accuracy is %0.6f' % train_accuracy)\n",
    "    confusion_M = confusion_matrix(train_predictions, train_Y)\n",
    "    print(confusion_M)\n",
    "    print(\"\\nTEST SET\")\n",
    "    test_predictions = model.predict(test_reduced)\n",
    "    test_accuracy = accuracy_score(test_predictions, test_Y)\n",
    "    print('The test set accuracy is %0.6f' % test_accuracy)\n",
    "    confusion_M = confusion_matrix(test_predictions, test_Y)\n",
    "    print(confusion_M)\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = train_model('svm', 2, 'linear', 1e1, 'whatever', 'whatever', 'whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = train_model('svm', 2, 'linear', 1e-4, 'whatever', 'whatever', 'whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = train_model('svm', 2, 'rbf', 1e1, 'whatever', 'whatever', 'whatever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = train_model('decision_tree', 2, 'whatever', 'whatever', 20, 100, 'whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = train_model('decision_tree', 2, 'whatever', 'whatever', 200, 3, 'whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(decision_tree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_model('random_forest', 2, 'whatever', 'whatever', 20, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_model('random_forest', 2, 'whatever', 'whatever', 200, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Revisitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = train_model('logistic_regression', 1000, 'whatever',\n",
    "                                  'whatever', 'whatever', 'whatever', 'whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = np.asarray(word_counter.get_feature_names())\n",
    "args = np.argsort(logistic_regression.coef_[0])\n",
    "for a in args:\n",
    "    print(\" %s: %0.4f\" % (feature_names[a], logistic_regression.coef_[0][a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchLogRegress(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PyTorchLogRegress, self).__init__()\n",
    "        self.linear_layer = torch.nn.Linear(5, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear_output = self.linear_layer(x)\n",
    "        logistic_output = torch.nn.functional.sigmoid(linear_output)\n",
    "        return(logistic_output)\n",
    "\n",
    "\n",
    "# Make it\n",
    "classifier = PyTorchLogRegress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduction = PCA(n_components = 5, random_state=123)\n",
    "dim_reduction.fit(train_word_counts.todense())\n",
    "train_reduced = dim_reduction.transform(train_word_counts.todense())\n",
    "test_reduced = dim_reduction.transform(test_word_counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_X = Variable(torch.from_numpy(train_reduced)).type(torch.FloatTensor)\n",
    "train_torch_Y = Variable(torch.from_numpy(train_Y)).type(torch.FloatTensor).unsqueeze(dim = 1)\n",
    "\n",
    "test_torch_X = Variable(torch.from_numpy(test_reduced)).type(torch.FloatTensor)\n",
    "test_torch_Y = Variable(torch.from_numpy(test_Y)).type(torch.FloatTensor).unsqueeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = classifier(train_torch_X)\n",
    "    test_predictions = classifier(test_torch_X)\n",
    "    \n",
    "    loss = loss_function(predictions, train_torch_Y)\n",
    "    test_loss = loss_function(test_predictions, test_torch_Y)\n",
    "    \n",
    "    loss_list.append(loss.data[0])\n",
    "    test_loss_list.append(test_loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list, label = 'train')\n",
    "plt.plot(test_loss_list, label = 'test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_prediction_list = []\n",
    "\n",
    "test_predictions = classifier(test_torch_X)\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    plain_prediction = test_predictions[i].data.numpy()[0]\n",
    "    if plain_prediction < 0.5:\n",
    "        plain_prediction_list.append(0)\n",
    "    else:\n",
    "        plain_prediction_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(plain_prediction_list, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(plain_prediction_list, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
